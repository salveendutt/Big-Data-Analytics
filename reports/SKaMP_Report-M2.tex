\documentclass[12pt,a4paper, hidelinks]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{fancyhdr}
\usepackage{indentfirst} % Add this line to enable first paragraph indentation
\usepackage{times} % Use Times New Roman font
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{float}

\setstretch{1.15} % Adjust the stretch factor as needed

\pagestyle{fancy}
\fancyhf{}  % Clear header and footer fields
\rfoot{\thepage}  % Place page number at the right bottom corner
\renewcommand{\headrulewidth}{0pt}  % Remove the header line


\begin{document}

% Title Page
\begin{titlepage}
    \centering
    \vspace*{0.5 cm}
    \includegraphics[width=0.20\textwidth]{images/logo.png}\par\vspace{1cm}
    {\scshape\LARGE Warsaw University of Technology \par}
    \vspace{1cm}
    {\scshape\Large Faculty of Mathematics and Information Science\par}
    \vspace{1.5cm}
    {\huge\bfseries Real-time fraudulent transactions detection \par}
    \vspace{1cm}
    {\Large\itshape Big Data Analytics\par}
    \vfill
    % \vspace{2cm}
    \begin{flushright}

    {\Large\textbf Salveen Singh Dutt (317298) \\ Karina Tiurina (335943) \\ Miko≈Çaj Malec (298828) \\ Patryk Prusak (305794) \par}
    \vfill
    {Supervisor\par}
    {\Large mgr inz. Jakub Abelski \par}
    
    \end{flushright}
    \vfill
    % \break
    {\large Warsaw 2024\par}
    \vspace{1cm}
\end{titlepage}

\newpage

% Table of contents
\tableofcontents
\newpage % Optional: Add a page break after the TOC

\section*{Introduction}
\addcontentsline{toc}{section}{Introduction}
\vspace{\baselineskip} % Add an empty line after the section title

The goal of this project is to plan and implement financial transactions processing system which identifies suspicious and fraudulent activity in real-time. Given the large volume of incoming data, the project will utilize big data technologies as well as advanced machine learning algorithms for anomaly detection.

GitHub repository: \href{https://github.com/salveendutt/Big-Data-Analytics}{https://github.com/salveendutt/Big-Data-Analytics}.

\subsection*{Updates for Milestone 2}
\addcontentsline{toc}{subsection}{Updates for Milestone 2}
The project architecture layout was updated to better follow the Lambda architecture. In particular: 
\begin{itemize}
    \item master data storage was moved to the Batch layer;
    \item included NoSQL database as a storage for data querying in the Serving layer;
    \item the architecture schema was updated to better represent the data flow. 
\end{itemize}

\newpage

\section{High level description}

The main idea is to implement an automatic transactions processing so that anytime a fraudulent activity occurs, the transaction is blocked for further manual review. The aim is to reduce financial losses of the end-users and to enhance the security of online payment, ensuring a safer experience for all customers.

There are two main end-users of the project: financial institutions (we will call them 'Managers') and their customers executing the payments. Although both categories can benefit from the solution, in our implementation we will mainly focus on Managers to limit additional data in storage.

The list below contains main features that we expect to implement for Managers:
\begin{enumerate}
    \item Fraudulent transactions are automatically highlighted so that it is easier to identify suspicious activity;
    \item The history of transactions is stored and available for later review;
    \item A dashboard with statistics of fraudulent activity is available and customizable for better localisation of issues (e.g. too large amount, unusual location);
    \item Anomaly-detection model is continuously updated so that fraud detection utilizes new historical data and is more accurate on future transactions;
    \item Data streaming processing and batch jobs are customizable so that the testing of model's performance is simplified.
\end{enumerate}


\newpage

\section{Data sources}

Due to strict security regulations on personal and financial data, it is quite challenging to find open source real transactions data both for model training and streaming. Therefore, available synthetic and anonymized datasets will be used. The table below contains description of the data sources. Each data source is described in more detail in the dedicated subsections.

\begin{table}[h!]
\centering
\begin{tabular}{|p{5cm}|p{4cm}|p{3cm}|p{2cm}|p{1.5cm}|}
\hline
\textbf{Data Source} & \textbf{Content} & \textbf{Volume} &  \textbf{Fraud, \%} & \textbf{Link} \\
\hline
1. Fraudulent Transactions Data &  Dataset for predicting fraudulent transactions for a financial company. &  6,362,620 rows and 10 columns (493.53 MB) & 0.13\% & \href{https://www.kaggle.com/datasets/chitwanmanchanda/fraudulent-transactions-data}{Kaggle} \\
\hline
2. Credit Card Fraud & Contains features with transactional context. & 1,000,000 transactions (58.9 MB) & 8.7\% & \href{https://www.openml.org/search?type=data&status=active&id=45955}{OpenML} \\
\hline
3. Credit Card Transactions Synthetic Data Generation & A collection of synthetic credit card transaction data. & 1,785,308 transactions; 5,000 customers; (153.66 MB) & 3\% & \href{https://www.kaggle.com/datasets/cgrodrigues/credit-card-transactions-synthetic-data-generation?select=transactions_df.csv}{Kaggle} \\
\hline
4. Credit Card Fraud Detection & Transactions made by credit cards in September 2013 by European cardholders. & 284,807 transactions (150.83 MB) & 0.17\% & \href{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud}{Kaggle} \\
\hline
\end{tabular}
\caption{Data sources}
\end{table}

Data-streaming API was implemented from scratch. The assumption was that it would use the above datasets; with a specified time-frame, it would choose a random transaction which was not used for training and push it for further processing. For the testing purposes, the probability of a fraudulent transaction will be set manually to some high enough constant value. Details description of the implented streaming API can be found in chapter 3.


\newpage

\subsection{Fraudulent Transactions Data (Kaggle)}

Dataset contains transactions for a financial company, indicating whether it is fraudulent or not. Data for the case is available in CSV format having 6362620 rows and 10 columns. Full column description is presented in the table 2.

\begin{table}[ht!]
\centering
\begin{tabular}{|p{4cm}|p{10cm}|p{2cm}|}
\hline
\textbf{Column} & \textbf{Content} & \textbf{Type} \\
\hline
step & maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation) & int \\
\hline
type & type of the transaction. Available values: CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER & str \\
\hline
amount & amount of the transaction in local currency & float \\
\hline
nameOrig & customer who started the transaction & str \\
\hline
oldbalanceOrg & initial balance before the transaction & float \\
\hline
newbalanceOrig & new balance after the transaction & float \\
\hline
nameDest & customer who is the recipient of the transaction & str \\
\hline
oldbalanceDest & initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants) & float \\
\hline
newbalanceDest & new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants) & float \\
\hline
isFraud & this is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system & int \\
\hline
isFlaggedFraud & the business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction & int \\
\hline
\end{tabular}
\caption{Columns description. Dataset 1: 'Fraudulent Transactions Data' from Kaggle}
\end{table}

The following transformation should be done to pass dataset to the ML models:
\begin{enumerate}
    \item 'type' column values CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER transformed to 1, 2, 3, 4 and 5 respectively;
    \item a new attribute 'isMerchant' is calculated. 1 if 'nameDest' starts with 'M', 0 - otherwise.
  \end{enumerate}

Generally, the dataset and clean and structured, no other preprocessing, except for the described above is necessary. Potential issue is that it is highly unbalanced (less than 1\% of fraud transactions), and contains quite small amount of features (6) for model training. 

\newpage

\subsection{Credit Card Fraud (OpenML)}

This dataset captures transaction patterns and behaviors that could indicate potential fraud in card transactions. The data is composed of several features designed to reflect the transactional context such as geographical location, transaction medium, and spending behavior relative to the user's history.



\begin{table}[ht!]
    \centering
    \begin{tabular}{|p{5.5cm}|p{8cm}|p{2cm}|}
    \hline
    \textbf{Column} & \textbf{Content} & \textbf{Type} \\
    \hline
    distance\_from\_home & This is a numerical feature representing the geographical distance in kilometers between the transaction location and the cardholder's home address. & float \\
    \hline
    distance\_from\_last\_transaction & This numerical attribute measures the distance in kilometers from the location of the last transaction to the current transaction location. & float \\
    \hline
    ratio\_to\_median\_purchase\_price & A numeric ratio that compares the transaction's price to the median purchase price of the user's transaction history. & float \\
    \hline
    repeat\_retailer & A binary attribute where '1' signifies that the transaction was conducted at a retailer previously used by the cardholder, and '0' indicates a new retailer. & [0, 1] \\
    \hline
    used\_chip & This binary feature indicates whether the transaction was made using a chip (1) or not (0). & [0, 1] \\
    \hline
    used\_pin\_number &  Another binary feature, where '1' signifies the use of a PIN number for the transaction, and '0' shows no PIN number was used. & [0, 1] \\
    \hline
    online\_order & This attribute identifies whether the purchase was made online ('1') or offline ('0'). & [0, 1] \\
    \hline
    fraud & A binary target variable indicating whether the transaction was fraudulent ('1') or not ('0'). & [0, 1] \\
    \hline
    \end{tabular}
    \caption{Columns description. Dataset 2: 'Credit\_Card\_Fraud\_' from OpenML}
\end{table}

The whole dataset is in the numeric form, therefore, no additional preprocessing is necessary. The only transformation is to rename the target variable to 'isFraud' To match other datasets format.
The target feature balance is much better than for the previous dataset: 8.7\% of fraud. Additional complication is that feature 'amount' is not available. A ratio to the median purchase of the same customer is provided instead.

\newpage

\subsection{Credit Card Transactions Synthetic Data Generation (Kaggle)}

TBD

\newpage

\subsection{Credit Card Fraud Detection (Kaggle)}

TBD

\newpage

\section{Data acquisition strategy}

TBD - (technology, libraries, API limitations)
describe streaming api

\section{Data transformation steps and data storage strategy}

TBD - (technology, format)

\section{Project architecture}

We are planning to implement the project based on Lambda Architecture. The main data processing will be divided into three layers:

\begin{enumerate}
    \item Speed Layer (streaming)
        \begin{itemize}
            \item Data preprocessing including transformation to a specific format;
            \item Real-time fraud detection on all of the incoming transactions.
        \end{itemize}
    \item Batch Layer
        \begin{itemize}
            \item Data processing and filtering for the model training
            \item ML model training with a fixed schedule (e.g. every 10 minutes)
        \end{itemize}
    \item Serving Layer
        \begin{itemize}
            \item Stores processed real-time and batch data in NoSQL for fast querying
            \item Client interface highlighting fraud transactions, accepting/blocking transactions;
            \item Data visualization with customizable filters
        \end{itemize}
\end{enumerate}

Figure 1 shows an outline of the project architecture.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{images/Architecture-M2.png}
    \caption{Project architecture}
    \label{fig:sunset}
\end{figure}

The following Big Data platforms will be used:
\begin{itemize}
    \item Apache NiFi: to collect and distribute the data from different sources;
    \item Apache Hive as a data storage;
    \item Apache Kafka: to work with the streaming data;
    \item Apache Spark: to make the batch processing and model training;
    \item MongoDB: to store prepared batch view and real-time views for fast quering;
    \item Apache Superset (to be agreed with the supervisor): for data analysis on the user-interface. If the service will not be approved, UI will be implemented from scratch using JS framework, e.g. React.
\end{itemize}

\section{Planned ML tasks}

TBD

\section{Planned batch and stream processing}

TBD - The planned use of batch and stream processing for individual tasks

\section{Planned way of presenting the results}

TBD - Planned way of presenting the results to end-users in the further part of the project


\newpage

\section{Tasks assignment}

The table below contains the list of team members and preliminary allocation of tasks to team members.

\begin{table}[htbp]
\centering
\begin{tabular}{|p{4cm}|p{6cm}|p{4cm}|}
\hline
\textbf{Team member} & \textbf{Tasks} & \textbf{Supporter} \\
\hline
Salveen Singh Dutt & Batch processing of the historical data for up-to-date model training (Batch Layer). & Karina Tiurina \\
\hline
Karina Tiurina & Fraud detection model training and fine-tuning; Data stream processing (Speed Layer). & Salveen Singh Dutt \\
\hline
Miko≈Çaj Malec & Data ingestion, collection and pre-processing. & Patryk Prusak  \\
\hline
Patryk Prusak & Data visualisation and configuration on the UI (Serving Layer). & Miko≈Çaj Malec \\
\hline
\end{tabular}
\caption{Tasks assignment}
\end{table}


\end{document}